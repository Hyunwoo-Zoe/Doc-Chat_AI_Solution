#!/usr/bin/env bash
# β‡Ά Qwen-30B-GPTQ λ¥Ό vLLM(OpenAI νΈν™)μΌλ΅ λ„μ°λ” μ¤ν¬λ¦½νΈ
set -euo pipefail

###############################################################################
# 0. μ‚¬μ©μ μ…λ ¥ (κΈ°λ³Έκ°’: GPU 0 / ν¬νΈ 12000)
###############################################################################
read -p "π–¥οΈ  Qwenμ„ λλ¦΄ GPU λ²νΈ(μ‰Όν‘λ΅ μ—¬λ¬ κ° κ°€λ¥, κΈ°λ³Έ 0): " GPU_VLLM
GPU_VLLM=${GPU_VLLM:-0}

read -p "π OpenAI νΈν™ ν¬νΈ λ²νΈ(κΈ°λ³Έ 12000): " PORT_VLLM
PORT_VLLM=${PORT_VLLM:-12000}

###############################################################################
# 1. κ²½λ΅ λ° λ³€μ
###############################################################################
MODEL_NAME="Qwen/Qwen3-30B-A3B-GPTQ-Int4"
BASE_DIR="$HOME/vllm-data-storage"
VENV_DIR="$BASE_DIR/vllm-venv"
HF_CACHE="$BASE_DIR/huggingface-cache"
VLLM_CACHE="$BASE_DIR/vllm-cache"
LOG_FILE="vllm.log"

mkdir -p "$BASE_DIR" "$HF_CACHE" "$VLLM_CACHE"

###############################################################################
# 2. Python κ°€μƒν™κ²½
###############################################################################
[[ -d "$VENV_DIR" ]] || python3 -m venv "$VENV_DIR"
source "$VENV_DIR/bin/activate"
VENV_PY="$VENV_DIR/bin/python"
echo "[vLLM venv] $($VENV_PY -V)"

###############################################################################
# 3. vLLM μ„¤μΉ(μ—†μΌλ©΄)
###############################################################################
if ! $VENV_PY - <<'PY' 2>/dev/null
import importlib, sys; sys.exit(0 if importlib.util.find_spec("vllm") else 1)
PY
then
  echo "[vLLM] ν¨ν‚¤μ§€ μ„¤μΉ μ¤‘β€¦"
  $VENV_PY -m pip install --upgrade pip wheel
  $VENV_PY -m pip install "vllm[serve]==0.4.2"
fi

###############################################################################
# 4. HF ν† ν° μ…λ ¥
###############################################################################
if [[ -z "${HUGGING_FACE_HUB_TOKEN:-}" ]]; then
  read -s -p "π”‘  HUGGING_FACE_HUB_TOKEN: " HUGGING_FACE_HUB_TOKEN; echo
  export HUGGING_FACE_HUB_TOKEN
fi
export HF_HOME="$HF_CACHE" VLLM_CACHE_ROOT="$VLLM_CACHE"

###############################################################################
# 5. μ„λ²„ κΈ°λ™
###############################################################################
export CUDA_VISIBLE_DEVICES="$GPU_VLLM"

echo -e "\n[π€] vLLM(Qwen) β†’ GPU $GPU_VLLM / PORT $PORT_VLLM"
nohup $VENV_PY -m vllm.entrypoints.openai.api_server \
      --model "$MODEL_NAME" \
      --enable_expert_parallel \
      --trust-remote-code \
      --host 0.0.0.0 \
      --port "$PORT_VLLM"  > "$LOG_FILE" 2>&1 &

PID=$!
printf "[β›] PID %s β€“ λ΅λ”© μ¤‘β€¦\n" "$PID"

for _ in {1..150}; do
  sleep 2
  lsof -i :"$PORT_VLLM" &>/dev/null && { echo "β… Ready! tail -f $LOG_FILE"; exit 0; }
done
echo "β 300μ΄ λ‚΄μ— μ‹μ‘λμ§€ μ•μ•μµλ‹λ‹¤. tail -f $LOG_FILE λ΅ ν™•μΈν•μ„Έμ”."
exit 1

