#!/usr/bin/env bash
# β‡Ά μΊ΅μ…”λ‹ λ©€ν‹°λ¨λ‹¬ λ¨λΈ(LLaVA λ“±)μ„ vLLM(OpenAI νΈν™)μΌλ΅ λ„μ°λ” μ¤ν¬λ¦½νΈ
set -euo pipefail

###############################################################################
# 0. μ‚¬μ©μ μ…λ ¥ (κΈ°λ³Έκ°’: GPU 0 / ν¬νΈ 12001 / λ¨λΈ llava-hf/llava-1.6-7b-hf)
###############################################################################
read -p "π–¥οΈ  μ‚¬μ©ν•  GPU λ²νΈ(μ‰Όν‘λ΅ μ—¬λ¬ κ° κ°€λ¥, κΈ°λ³Έ 0): " GPU_VLM
GPU_VLM=${GPU_VLM:-0}

read -p "π OpenAI νΈν™ ν¬νΈ λ²νΈ(κΈ°λ³Έ 12001): " PORT_VLM
PORT_VLM=${PORT_VLM:-12001}

# read -p "π§  λ¨λΈ μ΄λ¦„(HF repo λλ” λ΅μ»¬ κ²½λ΅, κΈ°λ³Έ llava-hf/llava-1.6-7b-hf): " MODEL_NAME
MODEL_NAME="llava-hf/llava-1.5-7b-hf"

###############################################################################
# 1. κ²½λ΅ λ° λ³€μ
###############################################################################
BASE_DIR="$HOME/vllm-data-storage"
VENV_DIR="$BASE_DIR/datastorage"
HF_CACHE="$BASE_DIR/huggingface-cache"
VLLM_CACHE="$BASE_DIR/vllm-cache"
LOG_FILE="vlm-vllm.log"

mkdir -p "$BASE_DIR" "$HF_CACHE" "$VLLM_CACHE"

###############################################################################
# 2. Python κ°€μƒν™κ²½
###############################################################################
[[ -d "$VENV_DIR" ]] || python3 -m venv "$VENV_DIR"
source "$VENV_DIR/bin/activate"
VENV_PY="$VENV_DIR/bin/python"
echo "[vLLM venv] $($VENV_PY -V)"

###############################################################################
# 3. vLLM μ„¤μΉ(μ—†μΌλ©΄)
###############################################################################
# if ! $VENV_PY - <<'PY' 2>/dev/null
# import importlib, sys; sys.exit(0 if importlib.util.find_spec("vllm") else 1)
# PY
# then
#   echo "[vLLM] ν¨ν‚¤μ§€ μ„¤μΉ μ¤‘β€¦"
#   $VENV_PY -m pip install --upgrade pip wheel
#   $VENV_PY -m pip install "vllm[serve]==0.4.2"
# fi

###############################################################################
# 4. HF ν† ν° μ…λ ¥
###############################################################################
if [[ -z "${HUGGING_FACE_HUB_TOKEN:-}" ]]; then
  read -s -p "π”‘  HUGGING_FACE_HUB_TOKEN: " HUGGING_FACE_HUB_TOKEN; echo
  export HUGGING_FACE_HUB_TOKEN
fi
export HF_HOME="$HF_CACHE" VLLM_CACHE_ROOT="$VLLM_CACHE"

###############################################################################
# 5. μ„λ²„ κΈ°λ™
###############################################################################
export CUDA_VISIBLE_DEVICES="$GPU_VLM"
# FLASHINFER λ°±μ—”λ“ μ‚¬μ©μΌλ΅ xformers νΈν™μ„± λ¬Έμ  ν•΄κ²°
export VLLM_ATTENTION_BACKEND=FLASHINFER

echo -e "\n[π€] vLLM(VLM) β†’ GPU $GPU_VLM / PORT $PORT_VLM / MODEL $MODEL_NAME"
echo -e "[π”§] Attention Backend: FLASHINFER (xformers νΈν™μ„± λ¬Έμ  μ°ν)"
echo -e "[π’Ύ] Memory Optimization: KV cache size reduced, chunked prefill enabled"
nohup $VENV_PY -m vllm.entrypoints.openai.api_server \
      --model "$MODEL_NAME" \
      --trust-remote-code \
      --host 0.0.0.0 \
      --port "$PORT_VLM" \
      --gpu-memory-utilization 0.85 \
      --max-model-len 2048 \
      --max-num-batched-tokens 1024 \
      --enable-chunked-prefill  > "$LOG_FILE" 2>&1 &

PID=$!
printf "[β›] PID %s β€“ λ΅λ”© μ¤‘β€¦\n" "$PID"

for _ in {1..150}; do
  sleep 2
  lsof -i :"$PORT_VLM" &>/dev/null && { echo "β… Ready! tail -f $LOG_FILE"; exit 0; }
done
echo "β 300μ΄ λ‚΄μ— μ‹μ‘λμ§€ μ•μ•μµλ‹λ‹¤. tail -f $LOG_FILE λ΅ ν™•μΈν•μ„Έμ”."
exit 1

